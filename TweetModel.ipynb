{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec053684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from math import log, sqrt\n",
    "\n",
    "class TweetClassifier(object):\n",
    "    def __init__(self, trainData, method='tf-idf'):\n",
    "        self.tweets, self.labels = trainData['message'], trainData['label']\n",
    "        self.method = method\n",
    "\n",
    "    def train(self):\n",
    "        self.calc_TF_and_IDF()\n",
    "        if self.method == 'tf-idf':\n",
    "            self.calc_TF_IDF()\n",
    "        else:\n",
    "            self.calc_prob()\n",
    "\n",
    "    def calc_prob(self):\n",
    "        self.prob_depressive = dict()\n",
    "        self.prob_positive = dict()\n",
    "        for word in self.tf_depressive:\n",
    "            self.prob_depressive[word] = (self.tf_depressive[word] + 1) / (self.depressive_words + \\\n",
    "                                                                           len(list(self.tf_depressive.keys())))\n",
    "        for word in self.tf_positive:\n",
    "            self.prob_positive[word] = (self.tf_positive[word] + 1) / (self.positive_words + \\\n",
    "                                                                       len(list(self.tf_positive.keys())))\n",
    "        self.prob_depressive_tweet, self.prob_positive_tweet = self.depressive_tweets / self.total_tweets, self.positive_tweets / self.total_tweets\n",
    "\n",
    "    def calc_TF_and_IDF(self):\n",
    "        noOfMessages = self.tweets.shape[0]\n",
    "        self.depressive_tweets, self.positive_tweets = self.labels.value_counts()[1], self.labels.value_counts()[0]\n",
    "        self.total_tweets = self.depressive_tweets + self.positive_tweets\n",
    "        self.depressive_words = 0\n",
    "        self.positive_words = 0\n",
    "        self.tf_depressive = dict()\n",
    "        self.tf_positive = dict()\n",
    "        self.idf_depressive = dict()\n",
    "        self.idf_positive = dict()\n",
    "        for i in range(noOfMessages):\n",
    "            message_processed = process_message(self.tweets.iloc[i])\n",
    "            count = list()  # To keep track of whether the word has ocured in the message or not.\n",
    "            # For IDF\n",
    "            for word in message_processed:\n",
    "                if self.labels.iloc[i]:\n",
    "                    self.tf_depressive[word] = self.tf_depressive.get(word, 0) + 1\n",
    "                    self.depressive_words += 1\n",
    "                else:\n",
    "                    self.tf_positive[word] = self.tf_positive.get(word, 0) + 1\n",
    "                    self.positive_words += 1\n",
    "                if word not in count:\n",
    "                    count += [word]\n",
    "            for word in count:\n",
    "                if self.labels.iloc[i]:\n",
    "                    self.idf_depressive[word] = self.idf_depressive.get(word, 0) + 1\n",
    "                else:\n",
    "                    self.idf_positive[word] = self.idf_positive.get(word, 0) + 1\n",
    "            pickle_out = open(\"data2.pickle\",\"wb\")\n",
    "            pickle.dump(self.depressive_words,pickle_out)\n",
    "            pickle.dump(self.positive_words,pickle_out)\n",
    "            pickle_out.close()\n",
    "\n",
    "    def calc_TF_IDF(self):\n",
    "        self.prob_depressive = dict()\n",
    "        self.prob_positive = dict()\n",
    "        self.sum_tf_idf_depressive = 0\n",
    "        self.sum_tf_idf_positive = 0\n",
    "        for word in self.tf_depressive:\n",
    "            self.prob_depressive[word] = (self.tf_depressive[word]) * log(\n",
    "                (self.depressive_tweets + self.positive_tweets) \\\n",
    "                / (self.idf_depressive[word] + self.idf_positive.get(word, 0)))\n",
    "            self.sum_tf_idf_depressive += self.prob_depressive[word]\n",
    "        for word in self.tf_depressive:\n",
    "            self.prob_depressive[word] = (self.prob_depressive[word] + 1) / (\n",
    "                        self.sum_tf_idf_depressive + len(list(self.prob_depressive.keys())))\n",
    "\n",
    "        for word in self.tf_positive:\n",
    "            self.prob_positive[word] = (self.tf_positive[word]) * log((self.depressive_tweets + self.positive_tweets) \\\n",
    "                                                                      / (self.idf_depressive.get(word, 0) +\n",
    "                                                                         self.idf_positive[word]))\n",
    "            self.sum_tf_idf_positive += self.prob_positive[word]\n",
    "        for word in self.tf_positive:\n",
    "            self.prob_positive[word] = (self.prob_positive[word] + 1) / (\n",
    "                        self.sum_tf_idf_positive + len(list(self.prob_positive.keys())))\n",
    "        self.prob_depressive_tweet, self.prob_positive_tweet = self.depressive_tweets / self.total_tweets, self.positive_tweets / self.total_tweets\n",
    "\n",
    "        pickle_out = open(\"data1.pickle\",\"wb\")\n",
    "        pickle.dump(self.prob_depressive,pickle_out)\n",
    "        pickle.dump(self.sum_tf_idf_depressive,pickle_out)\n",
    "        pickle.dump(self.prob_positive,pickle_out)\n",
    "        pickle.dump(self.sum_tf_idf_positive,pickle_out)\n",
    "        pickle.dump(self.prob_depressive_tweet,pickle_out)\n",
    "        pickle.dump(self.prob_positive_tweet,pickle_out)\n",
    "        pickle_out.close()\n",
    "\n",
    "    def classify(self, processed_message,method):\n",
    "\n",
    "        pickle_in = open(\"data1.pickle\",\"rb\")\n",
    "        prob_depressive = pickle.load(pickle_in)\n",
    "        sum_tf_idf_depressive = pickle.load(pickle_in)\n",
    "        prob_positive = pickle.load(pickle_in)\n",
    "        sum_tf_idf_positive = pickle.load(pickle_in)\n",
    "        prob_depressive_tweet = pickle.load(pickle_in)\n",
    "        prob_positive_tweet = pickle.load(pickle_in)\n",
    "\n",
    "        pickle_in = open(\"data2.pickle\",\"rb\")\n",
    "        depressive_words = pickle.load(pickle_in)\n",
    "        positive_words = pickle.load(pickle_in)\n",
    "\n",
    "        pDepressive, pPositive = 0, 0.\n",
    "\n",
    "        for word in processed_message:\n",
    "            if word in prob_depressive:\n",
    "                pDepressive += log(prob_depressive[word])\n",
    "            else:\n",
    "                if method == 'tf-idf':\n",
    "                    pDepressive -= log(sum_tf_idf_depressive + len(list(prob_depressive.keys())))\n",
    "                else:\n",
    "                    pDepressive -= log(depressive_words + len(list(prob_depressive.keys())))\n",
    "            if word in prob_positive:\n",
    "                pPositive += log(prob_positive[word])\n",
    "            else:\n",
    "                if method == 'tf-idf':\n",
    "                    pPositive -= log(sum_tf_idf_positive + len(list(prob_positive.keys())))\n",
    "                else:\n",
    "                    pPositive -= log(positive_words + len(list(prob_positive.keys())))\n",
    "            pDepressive += log(prob_depressive_tweet)\n",
    "            pPositive += log(prob_positive_tweet)\n",
    "        return pDepressive >= pPositive\n",
    "\n",
    "    def predict(self, testData,method):\n",
    "        result = dict()\n",
    "        for (i, message) in enumerate(testData):\n",
    "            processed_message = process_message(message)\n",
    "            result[i] = int(self.classify(processed_message,method))\n",
    "        return result\n",
    "\n",
    "def process_message(message, lower_case = True, stem = True, stop_words = True, gram = 2):\n",
    "    if lower_case:\n",
    "        message = message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    if gram > 1:\n",
    "        w = []\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            w += [' '.join(words[i:i + gram])]\n",
    "        return w\n",
    "    if stop_words:\n",
    "        sw = stopwords.words('english')\n",
    "        words = [word for word in words if word not in sw]\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a169ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
